{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa6833a5-ae09-47f8-8ce0-7e262093ad53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ***DataFrame en PySpark***\n",
    "\n",
    "Un DataFrame es una colección de datos estructurados que se pueden manipular utilizando operaciones como selección, filtrado, agregación y transformación, todo de manera distribuida. Esta estructura es optimizada por el motor de Apache Spark, lo que permite el procesamiento eficiente y paralelo de grandes conjuntos de datos.\n",
    "\n",
    "##**Características principales de un DataFrame en PySpark**:\n",
    "Distribuido: Un DataFrame en PySpark se distribuye entre diferentes nodos de un clúster de Spark, lo que permite procesar grandes cantidades de datos de manera paralela y eficiente.\n",
    "\n",
    "## **Estructura de columnas y filas**: \n",
    "Similar a una tabla en una base de datos relacional, cada DataFrame tiene columnas con tipos de datos específicos (como String, Integer, Double, etc.) y filas que contienen los datos.\n",
    "\n",
    "## **Optimización:** \n",
    "Los DataFrames en PySpark están optimizados mediante el Catalyst Optimizer, que mejora el rendimiento de las consultas a través de transformaciones internas de código.\n",
    "\n",
    "## **Lenguaje SQL:**\n",
    "Puedes utilizar consultas SQL directamente sobre un DataFrame en PySpark, ya que internamente se convierte en un plan de ejecución SQL que es procesado por Spark.\n",
    "\n",
    "## **Interoperabilidad:** \n",
    "Los DataFrames en PySpark pueden ser creados a partir de diferentes fuentes de datos, como archivos CSV, JSON, Parquet, bases de datos, y más. También puedes convertirlos a otros formatos o sistemas de procesamiento.\n",
    "\n",
    "## **Transformaciones y acciones:** \n",
    "Los DataFrames permiten realizar transformaciones (como select(), filter(), groupBy()) y acciones (como show(), collect(), count()) sobre los datos, lo que facilita el análisis de grandes volúmenes de información.\n",
    "\n",
    "## **Inmutable**\n",
    "La inmutabilidad en PySpark es un concepto clave cuando trabajas con DataFrames. Un DataFrame en PySpark es inmutable, lo que significa que una vez creado, no se puede modificar directamente. En otras palabras, cualquier operación que realices sobre un DataFrame no cambia el DataFrame original, sino que crea un nuevo DataFrame como resultado de la operación\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "```python\n",
    "# Crear un DataFrame\n",
    "df = spark.createDataFrame([(1, \"Alice\", 30), (2, \"Bob\", 25)], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Filtramos el DataFrame (crea un nuevo DataFrame)\n",
    "filtered_df = df.filter(df.age > 25)\n",
    "\n",
    "# Mostramos el DataFrame original\n",
    "df.show()\n",
    "\n",
    "# Mostramos el DataFrame filtrado\n",
    "filtered_df.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc581add-4231-43e2-ac0f-264ce2f443ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f266e25-0f8f-47f5-8f68-eae4a297a980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de tuplas que contiene información sobre los empleados: id, nombre, departamento y salario.\n",
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "       (2, \"BBB\", \"dept1\", 1100),\n",
    "       (3, \"CCC\", \"dept1\", 3000),\n",
    "       (4, \"DDD\", \"dept1\", 1500),\n",
    "       (5, \"EEE\", \"dept2\", 8000),\n",
    "       (6, \"FFF\", \"dept2\", 7200),\n",
    "       (7, \"GGG\", \"dept3\", 7100),\n",
    "       (8, \"HHH\", \"dept3\", 3700),\n",
    "       (9, \"III\", \"dept3\", 4500),\n",
    "       (10, \"JJJ\", \"dept5\", 3400)]\n",
    "\n",
    "# Lista de tuplas que contiene la relación entre el identificador del departamento y su nombre\n",
    "dept = [(\"dept1\", \"Department - 1\"),  \n",
    "        (\"dept2\", \"Department - 2\"),  \n",
    "        (\"dept3\", \"Department - 3\"),  \n",
    "        (\"dept4\", \"Department - 4\")]  \n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista de empleados 'emp', especificando los nombres de las columnas\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista de departamentos 'dept', especificando los nombres de las columnas\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad6d91c-2856-41b4-ac29-362b6f2d9f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n|  2| BBB|dept1|  1100|\n|  3| CCC|dept1|  3000|\n|  4| DDD|dept1|  1500|\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n|  8| HHH|dept3|  3700|\n|  9| III|dept3|  4500|\n| 10| JJJ|dept5|  3400|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f09a86dd-32c2-4fb5-bd45-9e71de82f995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metodos\n",
    "\n",
    "### Count() \n",
    "Retorna la Cantidad de Registros del Dataframe\n",
    "### Columns()\n",
    "Retorna los Nombres de las Columnas del DataFrame\n",
    "### dtypes\n",
    "Retorna el tipo de Dato de Cada Columna del Dataframe\n",
    "### schema\n",
    "Retorna la estructura del Dataframe almacenado en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275fa84a-dd81-43a3-ade4-1f604096eb60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n['id', 'name', 'dept', 'salary']\n[('id', 'bigint'), ('name', 'string'), ('dept', 'string'), ('salary', 'bigint')]\nStructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('dept', StringType(), True), StructField('salary', LongType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(df.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1546b00-cbd0-44e9-85fb-7aaaed69abcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El metodo `printSchema` permite mostrar de una manera mas legible la estructura del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbb2a57-ba71-42ef-8ea5-d72d27cf278c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2baa104a-4a41-424b-b5a0-40875806985c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `Select`\n",
    "Permite Seleccionar columnas de un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11b84cd-908e-414e-8b18-facb873f43c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n| id|name|\n+---+----+\n|  1| AAA|\n|  2| BBB|\n|  3| CCC|\n|  4| DDD|\n|  5| EEE|\n|  6| FFF|\n|  7| GGG|\n|  8| HHH|\n|  9| III|\n| 10| JJJ|\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e9e47b-5aad-40dc-a67d-971aba860233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `Filter`\n",
    "El método `filter` nos permite filtrar datos de un DataFrame y podemos usarlo de diferentes formas:\n",
    "\n",
    "- **`df.filter(df['id'] == 1).show()`**: Usamos una **expresión estándar de Python** para acceder a una columna del DataFrame con **corchetes** (`df[\"id\"]`), lo cual es útil cuando se trabaja con nombres de columnas que contienen caracteres especiales o cuando se necesita mayor flexibilidad en el acceso a las columnas. Posteriormente, comparamos si el valor de la columna `id` es igual a 1.\n",
    "\n",
    "- **`df.filter(df.id == 1).show()`**: Usamos la notación de **punto (`.`)** para acceder directamente a la columna `id`. Este enfoque es más limpio y sencillo cuando los nombres de las columnas son simples y no contienen caracteres especiales, haciendo el código más conciso y legible.\n",
    "\n",
    "- **`df.filter(col(\"id\") == 1).show()`**: Usamos la función **`col()`** de **`pyspark.sql.functions`** para hacer referencia a las columnas del DataFrame. Este método es muy útil cuando se aplican transformaciones complejas o cuando el nombre de la columna contiene caracteres especiales. La función `col()` es especialmente útil cuando se trabajan con múltiples columnas o se usan funciones adicionales sobre las columnas, como agregaciones, condiciones complejas, etc.\n",
    "\n",
    "- **`df.filter(\"id = 1\").show()`**: Usamos la condición de comparación como una **cadena de texto**, lo que permite escribir la condición de una manera similar a cómo escribiríamos una consulta SQL. Este enfoque es útil si estamos familiarizados con SQL o queremos escribir condiciones de forma más compacta, especialmente cuando se utilizan expresiones de filtrado simples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c4e87e-57a1-4387-93d3-f2246f8b8c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n+---+----+-----+------+\n\n+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"id\"] == 1).show()\n",
    "df.filter(df.id == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307e3870-d0c0-4e57-937b-aa06224edc41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n+---+----+-----+------+\n\n+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"id\") == 1).show()\n",
    "df.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33b71b9a-e880-4bdc-a9a3-6993396c3a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `Drop`\n",
    "El método drop se utiliza para eliminar columnas de un DataFrame. Es importante recordar que los DataFrames en PySpark son inmutables, lo que significa que no se modifica el DataFrame original. En lugar de eso, el método drop crea un nuevo DataFrame que no contiene la columna (o las columnas) descartadas. El DataFrame original permanece intacto.\n",
    "\n",
    "#### Inmutabilidad: \n",
    "Como los DataFrames son inmutables, cualquier operación que modifique su estructura (como agregar o eliminar columnas) no afecta el DataFrame original. En cambio, se devuelve una nueva instancia del DataFrame con los cambios aplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52038b79-67fe-4282-9f9a-3e9df4e6c288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n|name| dept|salary|\n+----+-----+------+\n| AAA|dept1|  1000|\n| BBB|dept1|  1100|\n+----+-----+------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.drop(\"id\")\n",
    "newdf.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81f9047e-5a07-479d-af4b-2a5d65ac2578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Groupby y agg \n",
    "En PySpark, los métodos groupBy y .agg se utilizan para realizar operaciones de agrupación y agregación sobre los datos en un DataFrame. Esto es muy útil cuando trabajas con grandes volúmenes de datos y necesitas calcular estadísticas, resúmenes o aplicar funciones específicas a grupos de datos.\n",
    "\n",
    "### `Groupby`\n",
    "El método groupBy se utiliza para agrupar las filas del DataFrame según los valores de una o más columnas. Es el primer paso en la creación de operaciones de agregación, como calcular promedios, sumas o contar registros dentro de cada grupo.\n",
    "\n",
    "```python\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = [(1, \"Alice\", \"Sales\", 1000),\n",
    "        (2, \"Bob\", \"Sales\", 1500),\n",
    "        (3, \"Charlie\", \"HR\", 1200),\n",
    "        (4, \"David\", \"HR\", 1300),\n",
    "        (5, \"Eve\", \"Sales\", 1100)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Agrupar por \"department\"\n",
    "grouped_df = df.groupBy(\"department\")\n",
    "grouped_df.show()\n",
    "```\n",
    "\n",
    "\n",
    "#### `.agg`\n",
    "El método .agg() se utiliza junto con groupBy para realizar operaciones de agregación sobre los grupos creados. Puedes aplicar varias funciones de agregación sobre una o más columnas, como contar, sumar, calcular el promedio, el valor máximo o mínimo, etc.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agrupar por \"department\" y calcular la suma de \"salary\" y el promedio de \"salary\"\n",
    "agg_df = df.groupBy(\"department\").agg(\n",
    "    F.sum(\"salary\").alias(\"total_salary\"),\n",
    "    F.avg(\"salary\").alias(\"average_salary\")\n",
    ")\n",
    "\n",
    "agg_df.show()\n",
    "\n",
    "```\n",
    "### Funciones comunes\n",
    "\n",
    "***count(): Cuenta el número de elementos en un grupo.***\n",
    "- `df.groupBy(\"department\").agg(F.count(\"salary\").alias(\"count_salary\")).show()`\n",
    "\n",
    "***sum(): Suma los valores de una columna.***\n",
    "- `df.groupBy(\"department\").agg(F.sum(\"salary\").alias(\"total_salary\")).show()`\n",
    "\n",
    "***avg(): Calcula el promedio de una columna.***\n",
    "- `df.groupBy(\"department\").agg(F.avg(\"salary\").alias(\"avg_salary\")).show()`\n",
    "\n",
    "***min(): Encuentra el valor mínimo de una columna.***\n",
    "- `df.groupBy(\"department\").agg(F.min(\"salary\").alias(\"min_salary\")).show()` \n",
    "\n",
    "***max(): Encuentra el valor máximo de una columna.***\n",
    "- `df.groupBy(\"department\").agg(F.max(\"salary\").alias(\"max_salary\")).show()`\n",
    "\n",
    " ***collect_list(): Devuelve una lista con todos los valores de la columna en cada grupo.***\n",
    "- `df.groupBy(\"department\").agg(F.collect_list(\"salary\").alias(\"salary_list\")).show()`\n",
    "\n",
    "***collect_set(): Devuelve un conjunto con los valores únicos de la columna en cada grupo.***\n",
    "- `df.groupBy(\"department\").agg(F.collect_set(\"salary\").alias(\"unique_salaries\")).show()\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc85069-5b62-44e7-835c-3a1a50a3e415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+----+----+------+\n| dept|count|  sum| max| min|   avg|\n+-----+-----+-----+----+----+------+\n|dept1|    4| 6600|3000|1000|1650.0|\n|dept2|    2|15200|8000|7200|7600.0|\n|dept3|    3|15300|7100|3700|5100.0|\n|dept5|    1| 3400|3400|3400|3400.0|\n+-----+-----+-----+----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy(\"dept\")\n",
    "    .agg(\n",
    "        count(\"salary\").alias(\"count\"),\n",
    "        sum(\"salary\").alias(\"sum\"),\n",
    "        max(\"salary\").alias(\"max\"),\n",
    "        min(\"salary\").alias(\"min\"),\n",
    "        avg(\"salary\").alias(\"avg\")\n",
    "        ).show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e14c70-c937-4ff6-9a35-0cb0ce1afa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### sort\n",
    "\n",
    "El método `sort` en *PySpark* se utiliza para ordenar los datos en un DataFrame según una o más columnas. El orden puede ser ascendente o descendente, y es una operación importante cuando necesitas organizar los datos para análisis o visualización.\n",
    "\n",
    "Por defecto, el método sort ordena los datos de manera ascendente (de menor a mayor), pero puedes especificar el orden para cada columna, ya sea ascendente o descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69491af-14b0-4591-b9dd-6b4a9ed0e0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n|  2| BBB|dept1|  1100|\n|  4| DDD|dept1|  1500|\n|  3| CCC|dept1|  3000|\n| 10| JJJ|dept5|  3400|\n+---+----+-----+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Ordenamiento por Defecto\n",
    "df.sort(\"salary\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535cba5f-2a64-4ea6-bae7-cdd6406e6f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n|  9| III|dept3|  4500|\n|  8| HHH|dept3|  3700|\n+---+----+-----+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Ordenamiento descendente\n",
    "df.sort(desc(\"salary\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6550cce3-b05e-46ad-b63a-5373912ccc38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Columnas Derivadas\n",
    "\n",
    "### `withColumn`\n",
    "El método `withColumn` se utiliza para crear nuevas columnas en un DataFrame o modificar columnas existentes a partir de las columnas ya presentes en el DataFrame. Es un método muy útil cuando necesitas realizar transformaciones o cálculos sobre los datos de una columna y agregar el resultado como una nueva columna en el DataFrame\n",
    "\n",
    "***Sintaxis Basica***\n",
    "`df = df.withColumn(\"new_column\", expression)`\n",
    "\n",
    "***Sintaxis Basica Calculo Varias columnas***\n",
    "\n",
    "```pyspark\n",
    "df = df.withColumn(\"new_column1\", expression1) \\\n",
    "       .withColumn(\"new_column2\", expression2) \\\n",
    "       .withColumn(\"new_column3\", expression3)\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4bc9ca-1d5a-4d3a-96fd-3e21c00fcd32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+\n| id|name| dept|salary|bonus|\n+---+----+-----+------+-----+\n|  1| AAA|dept1|  1000|100.0|\n|  2| BBB|dept1|  1100|110.0|\n|  3| CCC|dept1|  3000|300.0|\n|  4| DDD|dept1|  1500|150.0|\n|  5| EEE|dept2|  8000|800.0|\n|  6| FFF|dept2|  7200|720.0|\n|  7| GGG|dept3|  7100|710.0|\n|  8| HHH|dept3|  3700|370.0|\n|  9| III|dept3|  4500|450.0|\n| 10| JJJ|dept5|  3400|340.0|\n+---+----+-----+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b154c70-5192-4f9c-bdc8-cfae2935701f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Uso de `joins` en PySpark\n",
    "En PySpark, los **joins** permiten combinar dos o más DataFrames en base a columnas comunes, de manera similar a cómo funcionan los joins en SQL. Los joins son útiles cuando necesitas combinar datos que están en diferentes DataFrames pero comparten alguna relación (por ejemplo, una columna clave común).\n",
    "\n",
    "#### Tipos de Joins en PySpark\n",
    "\n",
    "Existen varios tipos de joins en PySpark:\n",
    "\n",
    "- **Inner Join**: Devuelve solo las filas donde existe una coincidencia en ambos DataFrames.\n",
    "- **Left Join (Left Outer Join)**: Devuelve todas las filas del DataFrame izquierdo y las filas coincidentes del DataFrame derecho. Si no hay coincidencia, los valores del DataFrame derecho serán `null`.\n",
    "- **Right Join (Right Outer Join)**: Similar al Left Join, pero devuelve todas las filas del DataFrame derecho.\n",
    "- **Full Join (Full Outer Join)**: Devuelve todas las filas de ambos DataFrames, con valores `null` donde no hay coincidencia.\n",
    "- **Cross Join**: Realiza un producto cartesiano entre ambos DataFrames, combinando cada fila del DataFrame izquierdo con todas las filas del DataFrame derecho.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94604d81-5f5b-4326-b92b-8f5624f67d79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inner Join\n",
    "\n",
    "***Sintaxis Basica***\n",
    "```python\n",
    "result = df1.join(df2, on=columna_comun, how=\"tipo_de_join\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c32fd47-805b-422f-9726-f65f5b00943b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n| id|name| dept|salary|   id|          name|\n+---+----+-----+------+-----+--------------+\n|  1| AAA|dept1|  1000|dept1|Department - 1|\n|  2| BBB|dept1|  1100|dept1|Department - 1|\n|  3| CCC|dept1|  3000|dept1|Department - 1|\n|  4| DDD|dept1|  1500|dept1|Department - 1|\n|  5| EEE|dept2|  8000|dept2|Department - 2|\n|  6| FFF|dept2|  7200|dept2|Department - 2|\n|  7| GGG|dept3|  7100|dept3|Department - 3|\n|  8| HHH|dept3|  3700|dept3|Department - 3|\n|  9| III|dept3|  4500|dept3|Department - 3|\n+---+----+-----+------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22f6f85a-dc47-480f-a515-1ee2c62faf78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Left Join\n",
    "\n",
    "***Sintaxis Basica***\n",
    "```python\n",
    "result = df1.join(df2, on=\"id\", how=\"left\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a063a4-aa2e-470f-be45-a0e1b999230f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-----+--------------+\n| id|name| dept|salary|   id|          name|\n+---+----+-----+------+-----+--------------+\n|  1| AAA|dept1|  1000|dept1|Department - 1|\n|  2| BBB|dept1|  1100|dept1|Department - 1|\n|  3| CCC|dept1|  3000|dept1|Department - 1|\n|  4| DDD|dept1|  1500|dept1|Department - 1|\n|  5| EEE|dept2|  8000|dept2|Department - 2|\n|  6| FFF|dept2|  7200|dept2|Department - 2|\n|  7| GGG|dept3|  7100|dept3|Department - 3|\n|  8| HHH|dept3|  3700|dept3|Department - 3|\n| 10| JJJ|dept5|  3400| null|          null|\n|  9| III|dept3|  4500|dept3|Department - 3|\n+---+----+-----+------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb453350-62be-47cc-8da8-e6459a05274b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### right Join\n",
    "\n",
    "***Sintaxis Basica***\n",
    "```python\n",
    "result = df1.join(df2, on=\"id\", how=\"right\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e0ad0c-d2e3-49eb-9165-d62568f831db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n|  id|name| dept|salary|   id|          name|\n+----+----+-----+------+-----+--------------+\n|   4| DDD|dept1|  1500|dept1|Department - 1|\n|   3| CCC|dept1|  3000|dept1|Department - 1|\n|   2| BBB|dept1|  1100|dept1|Department - 1|\n|   1| AAA|dept1|  1000|dept1|Department - 1|\n|   6| FFF|dept2|  7200|dept2|Department - 2|\n|   5| EEE|dept2|  8000|dept2|Department - 2|\n|   9| III|dept3|  4500|dept3|Department - 3|\n|   8| HHH|dept3|  3700|dept3|Department - 3|\n|   7| GGG|dept3|  7100|dept3|Department - 3|\n|null|null| null|  null|dept4|Department - 4|\n+----+----+-----+------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c723f5d-22ed-46a2-8001-be41e071b0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### outer Join\n",
    "\n",
    "***Sintaxis Basica***\n",
    "```python\n",
    "result = df1.join(df2, on=\"id\", how=\"outer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf9d17a6-01a8-49dc-adee-171d5cccf402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+-----+--------------+\n|  id|name| dept|salary|   id|          name|\n+----+----+-----+------+-----+--------------+\n|   1| AAA|dept1|  1000|dept1|Department - 1|\n|   2| BBB|dept1|  1100|dept1|Department - 1|\n|   3| CCC|dept1|  3000|dept1|Department - 1|\n|   4| DDD|dept1|  1500|dept1|Department - 1|\n|   5| EEE|dept2|  8000|dept2|Department - 2|\n|   6| FFF|dept2|  7200|dept2|Department - 2|\n|   7| GGG|dept3|  7100|dept3|Department - 3|\n|   8| HHH|dept3|  3700|dept3|Department - 3|\n|   9| III|dept3|  4500|dept3|Department - 3|\n|null|null| null|  null|dept4|Department - 4|\n|  10| JJJ|dept5|  3400| null|          null|\n+----+----+-----+------+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fecd6706-1f10-469f-a771-c046b558d653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uso de SQL en PySpark\n",
    "PySpark permite usar SQL de manera sencilla para consultar DataFrames mediante la integración de `SparkSession` con su motor de SQL. Puedes usar el método `spark.sql()` para ejecutar consultas SQL sobre los DataFrames que han sido registrados como **vistas temporales** (temporary views).\n",
    "\n",
    "## Configuración inicial\n",
    "\n",
    "Antes de usar SQL, es necesario registrar los DataFrames como vistas temporales. Una vez que se registre el DataFrame como vista, puedes hacer consultas SQL sobre él como si fuera una tabla en una base de datos tradicional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def4fee6-4af7-4fdc-befe-dd712f496a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Uso de SQL en PySpark: Registrar DataFrame como Vista Temporal\n",
    "\n",
    "En PySpark, puedes registrar un **DataFrame** como una **vista temporal** para poder ejecutar consultas SQL sobre él. Esta es una funcionalidad muy útil cuando quieres trabajar con PySpark de una manera más similar a cómo trabajas en bases de datos tradicionales usando SQL.\n",
    "\n",
    "## Registro del DataFrame como Vista Temporal\n",
    "\n",
    "Para registrar un **DataFrame** como una vista temporal, utilizamos el método `createOrReplaceTempView()`. Esto nos permite ejecutar consultas SQL sobre el DataFrame como si fuera una tabla en una base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b17b29b-1818-4e68-b48c-c446620b1f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "spark.sql(\"select * from temp_table where id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caabebdf-33f3-46df-a9be-c3f2b8ccff2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n|  5|\n|  4|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct id from temp_table\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d74621d-b1ed-4b12-b53b-2aedc99dc1da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  3| CCC|dept1|  3000|\n|  4| DDD|dept1|  1500|\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n|  8| HHH|dept3|  3700|\n|  9| III|dept3|  4500|\n| 10| JJJ|dept5|  3400|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from temp_table where salary >= 1500\").show(10)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Fundamentos Spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
