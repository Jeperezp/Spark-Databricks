{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf71603-13f9-49b3-a105-702a3c62d741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fundamentos de Apache Spark: Funciones avanzadas\n",
    "\n",
    "funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871226dc-e454-498a-8180-d6946b661e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bfc0f98-490b-40ef-b56f-09e7e4479db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de tuplas que contiene información sobre los empleados: id, nombre, departamento y salario.\n",
    "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
    "    (2, \"BBB\", \"dept1\", 1100),\n",
    "    (3, \"CCC\", \"dept1\", 3000),\n",
    "    (4, \"DDD\", \"dept1\", 1500),\n",
    "    (5, \"EEE\", \"dept2\", 8000),\n",
    "    (6, \"FFF\", \"dept2\", 7200),\n",
    "    (7, \"GGG\", \"dept3\", 7100),\n",
    "    (None, None, None, 7500),\n",
    "    (9, \"III\", None, 4500),\n",
    "    (10, None, \"dept5\", 2500)]\n",
    "\n",
    "# Lista de tuplas que contiene la relación entre el identificador del departamento y su nombre\n",
    "dept = [(\"dept1\", \"Department - 1\"),\n",
    "        (\"dept2\", \"Department - 2\"),\n",
    "        (\"dept3\", \"Department - 3\"),\n",
    "        (\"dept4\", \"Department - 4\")\n",
    "       ]\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista de empleados 'emp', especificando los nombres de las columnas\n",
    "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "\n",
    "# Crear un DataFrame de Spark a partir de la lista de departamentos 'dept', especificando los nombres de las columnas\n",
    "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c359ff13-f591-4998-a0d7-759d3704e680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creacion de vista Temporal\n",
    "df.createOrReplaceTempView(\"empdf\")\n",
    "deptdf.createOrReplaceTempView(\"deptdf\")\n",
    "\n",
    "# Guarda en una Tabla HIVE.\n",
    "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
    "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672e46bb-8d1b-4ddd-b8c7-9f3f31c4fac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `.cache()` y `.persist()` en PySpark\n",
    "\n",
    "En PySpark, los métodos `.cache()` y `.persist()` son técnicas de optimización que permiten almacenar los datos de un **DataFrame** o **RDD** para su reutilización, evitando cálculos redundantes. Estos métodos se utilizan cuando se realizan múltiples operaciones sobre los mismos datos, lo cual mejora el rendimiento al no tener que recalcular el mismo resultado varias veces.\n",
    "\n",
    "## `.cache()`\n",
    "\n",
    "### Concepto\n",
    "\n",
    "El método `.cache()` se utiliza para almacenar un **DataFrame** o **RDD** en **memoria**. Este almacenamiento permite acceder rápidamente a los datos durante las siguientes operaciones, ya que no es necesario volver a leer los datos desde el origen o recalcular los resultados de las operaciones previas.\n",
    "\n",
    "Cuando se llama al método `.cache()`, PySpark almacena el **DataFrame** o **RDD** en la **memoria RAM** de los nodos del clúster. Si hay suficiente memoria disponible, las siguientes operaciones sobre esos datos serán mucho más rápidas.\n",
    "\n",
    "**Nota**: `.cache()` es simplemente una forma específica de usar `.persist()` con el nivel de almacenamiento `MEMORY_ONLY`, es decir, almacena los datos solo en memoria.\n",
    "\n",
    "### Cuándo usar `.cache()`\n",
    "\n",
    "- **Optimización en operaciones repetitivas**: Se recomienda usar `.cache()` cuando sabes que los datos serán utilizados varias veces durante el proceso, evitando que PySpark tenga que volver a calcular o leer los mismos datos una y otra vez.\n",
    "- **Limitación**: Debido a que solo usa la memoria, si los datos no caben completamente en la memoria, se puede generar un error o un mal desempeño en el procesamiento.\n",
    "\n",
    "## `.persist()`\n",
    "\n",
    "### Concepto\n",
    "\n",
    "El método `.persist()` es más **flexible** que `.cache()`. Permite almacenar un **DataFrame** o **RDD** en varios niveles de almacenamiento que pueden incluir la memoria, el disco, o una combinación de ambos. Esto es útil cuando los datos son demasiado grandes para caber completamente en la memoria, o cuando se necesita un control más granular sobre dónde y cómo se almacenan los datos.\n",
    "\n",
    "A diferencia de `.cache()`, que utiliza solo la memoria, `.persist()` te permite elegir entre diferentes **niveles de almacenamiento** como:\n",
    "- **En memoria (MEMORY_ONLY)**: Almacena los datos solo en la memoria.\n",
    "- **En memoria y disco (MEMORY_AND_DISK)**: Almacena los datos en memoria y, si no caben, los guarda también en disco.\n",
    "- **Solo en disco (DISK_ONLY)**: Solo almacena los datos en disco, sin usar la memoria.\n",
    "\n",
    "### Cuándo usar `.persist()`\n",
    "\n",
    "- **Optimización avanzada**: Usar `.persist()` es ideal cuando se trabaja con grandes volúmenes de datos y se sabe que no cabe todo en memoria. Permite especificar el nivel de almacenamiento adecuado según la situación.\n",
    "- **Mejor control de recursos**: Proporciona mayor control sobre el uso de recursos, ya que puedes optar por persistir en disco, en memoria, o en una combinación de ambos.\n",
    "\n",
    "## Diferencias entre `.cache()` y `.persist()`\n",
    "\n",
    "- **Simplicidad vs Flexibilidad**: `.cache()` es más sencillo de usar, pero ofrece menos flexibilidad que `.persist()`. `.persist()` permite definir el nivel de almacenamiento, lo que resulta más útil cuando se necesita una gestión más avanzada de los recursos.\n",
    "- **Uso de memoria**: `.cache()` siempre intenta almacenar los datos en memoria, lo cual puede ser un problema si los datos son grandes. Mientras que `.persist()` ofrece más opciones como almacenamiento en disco o una combinación de ambos, lo que permite un manejo más eficiente de los recursos.\n",
    "\n",
    "## Resumen\n",
    "\n",
    "- **`.cache()`**: Es una forma rápida y sencilla de almacenar un **DataFrame** o **RDD** en memoria. Es ideal para cuando los datos caben en memoria y se van a reutilizar varias veces durante el proceso.\n",
    "- **`.persist()`**: Ofrece mayor flexibilidad al permitir almacenar los datos en diferentes niveles de almacenamiento (memoria, disco o combinación de ambos), lo cual es útil para trabajar con grandes volúmenes de datos.\n",
    "\n",
    "Ambos métodos son herramientas útiles en PySpark para optimizar el rendimiento, especialmente en procesos que requieren múltiples accesos a los mismos datos. Elegir entre `.cache()` y `.persist()` dependerá del tamaño de los datos y los recursos disponibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33aabd92-483c-419d-b9b7-7f58ed8ad4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\nDisk Used : True\n"
     ]
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e50b9b-af44-412b-9a85-b8bcd4f8d8cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cuando utilizamos la función `.cache()`, en versiones anteriores a Spark 2.1.x (hasta Spark 2.0.2), los datos se almacenan exclusivamente en memoria, utilizando el nivel de almacenamiento `MEMORY_ONLY`. Sin embargo, a partir de Spark 2.1.x, el comportamiento por defecto de `.cache()` cambió y ahora utiliza el nivel de almacenamiento `MEMORY_AND_DISK`, lo que significa que, si los datos no caben completamente en memoria, se almacenarán en disco.\n",
    "\n",
    "Si necesitamos un control más específico sobre los niveles de almacenamiento, podemos utilizar el método `.persist()`. Este método nos permite seleccionar entre diferentes niveles de almacenamiento, como por ejemplo, si deseamos almacenar los datos solo en memoria, podemos utilizar el nivel `MEMORY_ONLY`. \n",
    "\n",
    "A continuación se muestra un ejemplo de cómo especificar este comportamiento utilizando `.persist()`:\n",
    "\n",
    "```python\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "```\n",
    "\n",
    "De este modo, podemos elegir el nivel de almacenamiento que mejor se adapte a nuestras necesidades según el tamaño de los datos y los recursos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "268b45bb-de05-43c4-bf7c-d8afaf17dd3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fa4396-c4af-4bed-95b9-95e95f7b53d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used : True\nDisk Used : True\n"
     ]
    }
   ],
   "source": [
    "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
    "deptdf.count()\n",
    "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
    "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96931d49-a70b-4088-ac00-6085c1be2be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `.unpersist()` en PySpark\n",
    "\n",
    "El método `.unpersist()` en PySpark se utiliza para **liberar el almacenamiento** de un **DataFrame** o **RDD** que previamente ha sido almacenado en memoria o en disco mediante los métodos `.cache()` o `.persist()`. \n",
    "\n",
    "Cuando un DataFrame o RDD es persistido, los datos permanecen en la memoria o en el disco hasta que se realice explícitamente una acción para liberarlos. Esto es útil para evitar el consumo innecesario de memoria o recursos de disco una vez que los datos ya no son necesarios, mejorando así el rendimiento y la utilización de los recursos.\n",
    "\n",
    "## Concepto\n",
    "\n",
    "- **Liberación de almacenamiento**: `.unpersist()` elimina los datos almacenados en memoria o en disco para un DataFrame o RDD que fue previamente persistido o cacheado.\n",
    "- **Optimización de recursos**: Este método es útil cuando sabemos que no necesitaremos los datos de nuevo en el futuro cercano, lo que permite liberar memoria o espacio en disco para otras operaciones.\n",
    "\n",
    "## Sintaxis\n",
    "\n",
    "```python\n",
    "df.unpersist()\n",
    "\n",
    "```\n",
    "- `df`: El DataFrame o RDD del cual deseas liberar los recursos que ocupan memoria o disco.\n",
    "\n",
    "## Cuándo usar .unpersist()?\n",
    "- **Después de completar las operaciones:** Una vez que ya no se necesitan los datos en memoria o en disco, es recomendable llamar a .unpersist() para liberar esos recursos y evitar un uso innecesario de memoria, especialmente cuando se trabajan con grandes volúmenes de datos.\n",
    "\n",
    "- **Control de recursos:** Es importante liberar los recursos cuando los datos persistidos ya no se usarán en el futuro para optimizar el rendimiento de la aplicación y evitar posibles cuellos de botella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f738fa85-36ea-46a2-ab0d-85ab4f3a4cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
     ]
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5778c56f-daf9-49d2-a776-67e5a3595263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `.clearCache()` en PySpark\n",
    "\n",
    "El método `.clearCache()` en PySpark se utiliza para **eliminar todo el contenido almacenado en caché** en la sesión de Spark. Este método elimina los **DataFrames** o **RDDs** que han sido almacenados en memoria mediante los métodos `.cache()` o `.persist()`. \n",
    "\n",
    "Cuando se utiliza `.clearCache()`, se liberan los recursos de memoria que estaban siendo utilizados por los datos cacheados, lo que puede ser útil para liberar memoria y mejorar el rendimiento del clúster cuando ya no es necesario mantener esos datos en memoria.\n",
    "\n",
    "## Concepto\n",
    "\n",
    "- **Liberación de la caché**: `.clearCache()` elimina todos los datos almacenados en la caché en la sesión actual de Spark, liberando la memoria que estaba siendo utilizada.\n",
    "- **Optimización de recursos**: Es útil para evitar que los recursos de memoria se consuman innecesariamente cuando los datos almacenados en caché ya no son necesarios para las operaciones posteriores.\n",
    "\n",
    "## Sintaxis\n",
    "\n",
    "```python\n",
    "spark.catalog.clearCache()\n",
    "```\n",
    "## ¿Cuándo usar .clearCache()?\n",
    "- **Después de completar todas las operaciones:** Si has utilizado .cache() o .persist() para almacenar datos en memoria, pero ya no los necesitas, puedes usar .clearCache() para liberar la memoria y optimizar el uso de recursos.\n",
    "\n",
    "- **Cuando se trabaja con grandes volúmenes de datos:** Si los datos cacheados ocupan demasiada memoria o si estás experimentando cuellos de botella de memoria, .clearCache() es útil para liberar esa memoria y continuar con otras tareas.\n",
    "\n",
    "- **Mantener el clúster eficiente:** Si estás trabajando con múltiples aplicaciones o procesos en un clúster de Spark, liberar la caché puede mejorar la eficiencia global del sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6c82a9-1117-45d0-8680-807b186c9420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "178e524f-5899-4329-b61f-da1d354d96b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Expresiones SQL\n",
    "\n",
    "### `expr` y `selectExpr` en PySpark\n",
    "\n",
    "En PySpark, tanto **`expr`** como **`selectExpr`** son funciones que nos permiten ejecutar expresiones SQL dentro de un **DataFrame**. Ambas funciones ofrecen una forma flexible y poderosa de realizar transformaciones en los datos utilizando sintaxis SQL directamente en PySpark.\n",
    "\n",
    "### `expr`\n",
    "\n",
    "### Concepto\n",
    "\n",
    "La función **`expr`** se utiliza para **evaluar expresiones SQL** dentro de un **DataFrame**. Permite realizar operaciones sobre columnas utilizando la sintaxis SQL, como cálculos matemáticos, concatenaciones de texto, conversiones de tipos de datos, y más. Esta función es útil cuando queremos aplicar expresiones complejas en columnas o en nuevas columnas.\n",
    "\n",
    "### Sintaxis\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn(\"nueva_columna\", expr(\"columna1 + columna2\"))\n",
    "```\n",
    "### `selectExpr`\n",
    "\n",
    "### Concepto\n",
    "La función `selectExpr` es similar a `select()`, pero con la diferencia de que permite utilizar expresiones SQL dentro de su llamado. Se utiliza para seleccionar columnas o realizar transformaciones directamente con expresiones SQL. Esto es muy útil para realizar operaciones complejas o combinaciones de columnas sin necesidad de escribir múltiples líneas de código.\n",
    "\n",
    "### Sintaxis\n",
    "```pyspark\n",
    "#usarmos \"*\" para visualizar todos los campos de la tabla\n",
    "df.selectExpr(\"*\", \"expresión SQL\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad67857c-0b5d-49a7-83e7-5ade0e27db7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n|  id|name| dept|salary|salary_level|\n+----+----+-----+------+------------+\n|   1| AAA|dept1|  1000|  low_salary|\n|   2| BBB|dept1|  1100|  low_salary|\n|   3| CCC|dept1|  3000|  mid_salary|\n|   4| DDD|dept1|  1500|  low_salary|\n|   5| EEE|dept2|  8000| high_salary|\n|   6| FFF|dept2|  7200| high_salary|\n|   7| GGG|dept3|  7100| high_salary|\n|null|null| null|  7500| high_salary|\n|   9| III| null|  4500|  mid_salary|\n|  10|null|dept5|  2500|  mid_salary|\n+----+----+-----+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Intentemos categorizar el salario en Bajo, Medio y Alto según la categorización a continuación.\n",
    "\n",
    "# 0-2000: salario_bajo\n",
    "# 2001 - 5000: mid_salary\n",
    "#> 5001: high_salary\n",
    "\n",
    "cond = \"\"\"    CASE \n",
    "        WHEN salary > 5000 THEN 'high_salary'\n",
    "        WHEN salary > 2000 THEN 'mid_salary'\n",
    "        WHEN salary > 0 THEN 'low_salary'\n",
    "        ELSE 'invalid_salary'\n",
    "    END AS salary_level\"\"\"\n",
    "\n",
    "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23c8709-4368-41b2-8ae9-59031d21b864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n|  id|name| dept|salary|salary_level|\n+----+----+-----+------+------------+\n|   1| AAA|dept1|  1000|  low_salary|\n|   2| BBB|dept1|  1100|  low_salary|\n|   3| CCC|dept1|  3000|  mid_salary|\n|   4| DDD|dept1|  1500|  low_salary|\n|   5| EEE|dept2|  8000| high_salary|\n|   6| FFF|dept2|  7200| high_salary|\n|   7| GGG|dept3|  7100| high_salary|\n|null|null| null|  7500| high_salary|\n|   9| III| null|  4500|  mid_salary|\n|  10|null|dept5|  2500|  mid_salary|\n+----+----+-----+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.selectExpr(\"*\", cond)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f82808-ef64-4b3d-92fd-9391d2ab3c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Funciones Definidas por el Usuario (UDF) en PySpark\n",
    "\n",
    "En PySpark, una **función definida por el usuario (UDF)** es una función personalizada que se puede aplicar a las columnas de un **DataFrame**. Las UDFs permiten a los usuarios escribir funciones en Python (o en otros lenguajes como Java o Scala) y utilizarlas dentro de las operaciones de transformación de PySpark, extendiendo la funcionalidad de las funciones predefinidas que PySpark ofrece.\n",
    "\n",
    "## ¿Cuándo usar una UDF?\n",
    "\n",
    "Usamos UDFs cuando queremos realizar transformaciones que no están disponibles a través de las funciones integradas de PySpark, o cuando necesitamos una lógica de procesamiento más personalizada que no se puede expresar fácilmente con las funciones estándar de PySpark.\n",
    "\n",
    "## ¿Cómo funcionan las UDFs?\n",
    "\n",
    "Una **UDF** en PySpark recibe una o más columnas de un DataFrame, las procesa y devuelve un valor. Las UDFs son útiles para realizar operaciones sobre las columnas de un DataFrame cuando no se pueden lograr con las funciones predefinidas de PySpark. \n",
    "\n",
    "Las UDFs son especialmente útiles cuando:\n",
    "- Necesitas aplicar operaciones que no están disponibles en las funciones SQL o de PySpark.\n",
    "- Tienes una lógica compleja que debe ejecutarse en cada fila de un DataFrame.\n",
    "\n",
    "## Crear y usar una UDF\n",
    "\n",
    "### Pasos para crear una UDF en PySpark:\n",
    "\n",
    "1. **Definir una función Python**: Primero, defines una función normal en Python que realizará la operación que deseas aplicar.\n",
    "2. **Registrar la UDF**: Luego, registras la función en PySpark usando `udf()` y especificas el tipo de retorno.\n",
    "3. **Aplicar la UDF al DataFrame**: Después, usas la UDF dentro de operaciones como `select()`, `withColumn()`, etc.\n",
    "\n",
    "### Sintaxis\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Paso 1: Definir una función Python\n",
    "def my_function(value):\n",
    "    return value * 2\n",
    "\n",
    "# Paso 2: Registrar la función como UDF\n",
    "my_udf = udf(my_function, IntegerType())\n",
    "\n",
    "# Paso 3: Aplicar la UDF a una columna del DataFrame\n",
    "df.withColumn(\"new_column\", my_udf(df[\"existing_column\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d19c9476-eacc-44c3-a74b-00b6da764f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def detSalary_Level(sal:float)->str:\n",
    "    \"\"\"\n",
    "    Esta función determina el nivel salarial en función de un valor dado de salario.\n",
    "\n",
    "    Parámetros:\n",
    "    sal (float): El valor del salario a evaluar.\n",
    "\n",
    "    Retorna:\n",
    "    str: Un valor de tipo cadena que indica el nivel del salario:\n",
    "         - 'low_salary' si el salario es menor a 2000,\n",
    "         - 'mid_salary' si el salario es menor a 5000 (y mayor o igual a 2000),\n",
    "         - 'high_salary' si el salario es mayor o igual a 5000.\n",
    "    \"\"\"\n",
    "    level = None\n",
    "\n",
    "    if sal <2000:\n",
    "       level = 'low_salary'\n",
    "    elif sal < 5000:\n",
    "        level = 'mid_salary'\n",
    "    else:\n",
    "        level = 'high_salary'\n",
    "    return level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ca83b5-57eb-4ac7-ac1c-59a1a354f4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+------------+\n|  id|name| dept|salary|salary_level|\n+----+----+-----+------+------------+\n|   1| AAA|dept1|  1000|  low_salary|\n|   2| BBB|dept1|  1100|  low_salary|\n|   3| CCC|dept1|  3000|  mid_salary|\n|   4| DDD|dept1|  1500|  low_salary|\n|   5| EEE|dept2|  8000| high_salary|\n|   6| FFF|dept2|  7200| high_salary|\n|   7| GGG|dept3|  7100| high_salary|\n|null|null| null|  7500| high_salary|\n|   9| III| null|  4500|  mid_salary|\n|  10|null|dept5|  2500|  mid_salary|\n+----+----+-----+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Define la funcion udf\n",
    "sal_level = udf(detSalary_Level, StringType())\n",
    "#Aplica la funcion al Dataframe\n",
    "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0eafba8-e7f7-4fc0-b521-1a8967cb546e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Valores Nulos\n",
    "Spark tiene pocas funciones especificas para trabajar con valores Faltantes\n",
    "\n",
    "### Métodos de manejo de valores nulos en PySpark\n",
    "\n",
    "## `.isNull()`\n",
    "\n",
    "El método `.isNull()` en PySpark se usa para verificar si el valor de una columna es `NULL`. Devuelve una expresión booleana que puede ser utilizada en filtros para seleccionar filas donde el valor de una columna es nulo.\n",
    "\n",
    "- **Uso típico**: Se utiliza para filtrar filas donde una columna tiene valores nulos.\n",
    "- **Ejemplo**: `df.filter(df['column_name'].isNull())`\n",
    "\n",
    "## `.isNotNull()`\n",
    "\n",
    "El método `.isNotNull()` es el opuesto de `.isNull()`. Se utiliza para verificar si el valor de una columna no es `NULL`. Devuelve una expresión booleana que puede ser utilizada para seleccionar filas donde el valor de una columna es no nulo.\n",
    "\n",
    "- **Uso típico**: Se utiliza para filtrar filas donde una columna tiene valores no nulos.\n",
    "- **Ejemplo**: `df.filter(df['column_name'].isNotNull())`\n",
    "\n",
    "## `.fillna()`\n",
    "\n",
    "El método `.fillna()` se utiliza para reemplazar valores nulos (`NULL`) con un valor específico. Este método es útil cuando deseas llenar los valores nulos con un valor predeterminado, como 0, una cadena vacía, o cualquier otro valor.\n",
    "\n",
    "- **Uso típico**: Se utiliza para reemplazar valores nulos en una o varias columnas con un valor predeterminado.\n",
    "- **Sintaxis**:\n",
    "  ```python\n",
    "  df.fillna(value, subset=None)\n",
    "  ```\n",
    "- `value`: El valor con el que deseas reemplazar los valores nulos.\n",
    "\n",
    "- `subset:` (Opcional) Puedes especificar las columnas sobre las cuales deseas aplicar el reemplazo.\n",
    "- **Ejemplo**: `df.fillna({'column_name': 0, 'another_column': 'unknown'})`\n",
    "\n",
    "  ## `.dropna()`\n",
    "\n",
    "El método `.dropna()` en PySpark se utiliza para eliminar filas que contienen valores nulos (`NULL`) en el DataFrame. Este método es útil cuando deseas limpiar los datos y eliminar las filas que no tienen información completa en una o más columnas.\n",
    "\n",
    "### Sintaxis:\n",
    "```python\n",
    "df.dropna(how='any', thresh=None, subset=None)\n",
    "```\n",
    "### Parámetros:\n",
    "- `how:` Especifica cómo se deben eliminar las filas.\n",
    "\n",
    "  - **any**: Elimina las filas que tienen al menos un valor nulo en cualquier columna. Este es el valor predeterminado.\n",
    "  - **all**: Elimina las filas que tienen todos los valores nulos en todas las columnas.\n",
    "- `thresh`: (Opcional) Especifica el número mínimo de valores no nulos requeridos para que la fila se mantenga. Si una fila tiene menos de este número de valores no nulos, se eliminará.\n",
    "\n",
    "- `subset`: (Opcional) Permite especificar un subconjunto de columnas para evaluar si deben eliminarse las filas. Si no se especifica, se evalúan todas las columnas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6369cead-073d-4ba7-b46b-77848f848c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n|  id|name|dept|salary|\n+----+----+----+------+\n|null|null|null|  7500|\n|   9| III|null|  4500|\n+----+----+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ce7a7b-39ba-4323-962a-13c4120fc8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n|  2| BBB|dept1|  1100|\n|  3| CCC|dept1|  3000|\n|  4| DDD|dept1|  1500|\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n| 10|null|dept5|  2500|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.filter(df[\"dept\"].isNotNull())\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f5a18f-8712-4f8d-8b46-9b79be2e28d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+------+\n|  id|name|   dept|salary|\n+----+----+-------+------+\n|   1| AAA|  dept1|  1000|\n|   2| BBB|  dept1|  1100|\n|   3| CCC|  dept1|  3000|\n|   4| DDD|  dept1|  1500|\n|   5| EEE|  dept2|  8000|\n|   6| FFF|  dept2|  7200|\n|   7| GGG|  dept3|  7100|\n|null|null|INVALID|  7500|\n|   9| III|INVALID|  4500|\n|  10|null|  dept5|  2500|\n+----+----+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068000a5-8b48-44f3-9615-445c6e50cab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n|  2| BBB|dept1|  1100|\n|  3| CCC|dept1|  3000|\n|  4| DDD|dept1|  1500|\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna()\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c75dfae9-0baa-440a-bedd-5d313e4d86ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------+\n|  id|name| dept|salary|\n+----+----+-----+------+\n|   1| AAA|dept1|  1000|\n|   2| BBB|dept1|  1100|\n|   3| CCC|dept1|  3000|\n|   4| DDD|dept1|  1500|\n|   5| EEE|dept2|  8000|\n|   6| FFF|dept2|  7200|\n|   7| GGG|dept3|  7100|\n|null|null| null|  7500|\n|   9| III| null|  4500|\n|  10|null|dept5|  2500|\n+----+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Elimina todas las filas que contienen todos los valores nulos.\n",
    "newdf = df.dropna(how = \"all\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd25cb6-64b4-4bd5-9c3a-81bc72013f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+\n| id|name| dept|salary|\n+---+----+-----+------+\n|  1| AAA|dept1|  1000|\n|  2| BBB|dept1|  1100|\n|  3| CCC|dept1|  3000|\n|  4| DDD|dept1|  1500|\n|  5| EEE|dept2|  8000|\n|  6| FFF|dept2|  7200|\n|  7| GGG|dept3|  7100|\n| 10|null|dept5|  2500|\n+---+----+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "newdf = df.dropna(subset = \"dept\")\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f1c087-2f1a-45a5-8b83-e46371bb9ea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: 8"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Fundamentos Spark Fun Avan",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
